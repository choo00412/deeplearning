{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5f7fc15-2dab-4781-b4cc-b83f964dba68",
   "metadata": {},
   "source": [
    "<span style=\"font-size:20px;\"> 학번 : 2021136138\n",
    "<span style=\"font-size:20px;\"> 이름 : 추승연\n",
    "\r",
    ">\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4da3164f-2a91-4446-9bf5-1e2a34ab28a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch #PyTorch 모듈 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28ccda04-04e7-40ac-b448-e85696b87097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "cpu\n",
      "False\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "################################################## 1\n",
      "torch.int64\n",
      "cpu\n",
      "False\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "################################################## 2\n",
      "torch.Size([]) 0\n",
      "torch.Size([1]) 1\n",
      "torch.Size([5]) 1\n",
      "torch.Size([5, 1]) 2\n",
      "torch.Size([3, 2]) 2\n",
      "torch.Size([3, 2, 1]) 3\n",
      "torch.Size([3, 1, 2, 1]) 4\n",
      "torch.Size([3, 1, 2, 3]) 4\n",
      "torch.Size([3, 1, 2, 3, 1]) 5\n",
      "torch.Size([4, 5]) 2\n",
      "torch.Size([4, 1, 5]) 3\n",
      "torch.Size([4, 2, 3]) 3\n"
     ]
    }
   ],
   "source": [
    "# torch.Tensor class\n",
    "t1 = torch.Tensor([1, 2, 3], device='cpu')\n",
    "print(t1.dtype)   # 기본적으로 torch.Tensor()는 float32로 생성된다\n",
    "print(t1.device)  # 텐서가 생성된 장치는 'cpu'이다.\n",
    "print(t1.requires_grad)  # requires_grad 기본값은 False이다 (자동 미분을 사용하지 않음)\n",
    "print(t1.size())  # 텐서의 크기는 3이다.\n",
    "print(t1.shape)   # 텐서의 shape도 크기와 동일하게 3이다\n",
    "\n",
    "# 만약 GPU가 있다면 아래 코드로 디바이스 변경 가능\n",
    "# t1_cuda = t1.to(torch.device('cuda'))\n",
    "# 또는 t1.cuda()로도 가능\n",
    "t1_cpu = t1.cpu()  # 이미 CPU에 있으므로 변화가 없다\n",
    "\n",
    "print(\"#\" * 50, 1)\n",
    "\n",
    "# torch.tensor function\n",
    "t2 = torch.tensor([1, 2, 3], device='cpu')\n",
    "print(t2.dtype)  # torch.tensor()는 입력값에 따라 dtype을 결정하는데, 이 경우 int64이다.\n",
    "print(t2.device)  # t2는 'cpu'에서 생성되었다.\n",
    "print(t2.requires_grad)  # requires_grad는 False이다.\n",
    "print(t2.size())  # 텐서의 크기는 3이다.\n",
    "print(t2.shape)  # shape도 size와 동일하게 3이다.\n",
    "\n",
    "# GPU 디바이스 사용 시\n",
    "# t2_cuda = t2.to(torch.device('cuda'))\n",
    "# 또는 t2.cuda()로 가능\n",
    "t2_cpu = t2.cpu()  # CPU에 있으므로 변화가 없다.\n",
    "\n",
    "print(\"#\" * 50, 2)\n",
    "\n",
    "a1 = torch.tensor(1)  # 스칼라 값 하나로 텐서를 생성하면 shape는 빈 크기(0차원)가 된다.\n",
    "print(a1.shape, a1.ndim)  # 차원은 0이다.\n",
    "\n",
    "a2 = torch.tensor([1])  # 1차원 벡터로 생성되며, 크기는 [1]이다.\n",
    "print(a2.shape, a2.ndim)  # 차원은 1이다.\n",
    "\n",
    "a3 = torch.tensor([1, 2, 3, 4, 5])  # 1차원 벡터로 5개의 값을 가진 텐서이다.\n",
    "print(a3.shape, a3.ndim)  # 차원은 1이며, 크기는 [5]이다.\n",
    "\n",
    "a4 = torch.tensor([[1], [2], [3], [4], [5]])  # 2차원 텐서로, 5x1 크기를 가진다.\n",
    "print(a4.shape, a4.ndim)  # 차원은 2이며, 크기는 [5, 1]이다.\n",
    "\n",
    "a5 = torch.tensor([  # 2차원 텐서로 3x2 크기를 가진다.\n",
    "    [1, 2],\n",
    "    [3, 4],\n",
    "    [5, 6]\n",
    "])\n",
    "print(a5.shape, a5.ndim)  # 차원은 2이며, 크기는 [3, 2]이다.\n",
    "\n",
    "a6 = torch.tensor([  # 3차원 텐서로 3x2x1 크기를 가진다.\n",
    "    [[1], [2]],\n",
    "    [[3], [4]],\n",
    "    [[5], [6]]\n",
    "])\n",
    "print(a6.shape, a6.ndim)  # 차원은 3이며, 크기는 [3, 2, 1]이다.\n",
    "\n",
    "a7 = torch.tensor([  # 4차원 텐서로 3x1x2x1 크기를 가진다.\n",
    "    [[[1], [2]]],\n",
    "    [[[3], [4]]],\n",
    "    [[[5], [6]]]\n",
    "])\n",
    "print(a7.shape, a7.ndim)  # 차원은 4이며, 크기는 [3, 1, 2, 1]이다.\n",
    "\n",
    "a8 = torch.tensor([  # 4차원 텐서로 3x1x2x3 크기를 가진다.\n",
    "    [[[1, 2, 3], [2, 3, 4]]],\n",
    "    [[[3, 1, 1], [4, 4, 5]]],\n",
    "    [[[5, 6, 2], [6, 3, 1]]]\n",
    "])\n",
    "print(a8.shape, a8.ndim)  # 차원은 4이며, 크기는 [3, 1, 2, 3]이다.\n",
    "\n",
    "a9 = torch.tensor([  # 5차원 텐서로 3x1x2x3x1 크기를 가진다.\n",
    "    [[[[1], [2], [3]], [[2], [3], [4]]]],\n",
    "    [[[[3], [1], [1]], [[4], [4], [5]]]],\n",
    "    [[[[5], [6], [2]], [[6], [3], [1]]]]\n",
    "])\n",
    "print(a9.shape, a9.ndim)  # 차원은 5이며, 크기는 [3, 1, 2, 3, 1]이다.\n",
    "\n",
    "a10 = torch.tensor([  # 2차원 텐서로 4x5 크기를 가진다.\n",
    "    [1, 2, 3, 4, 5],\n",
    "    [1, 2, 3, 4, 5],\n",
    "    [1, 2, 3, 4, 5],\n",
    "    [1, 2, 3, 4, 5],\n",
    "])\n",
    "print(a10.shape, a10.ndim)  # 차원은 2이며, 크기는 [4, 5]이다.\n",
    "\n",
    "a10 = torch.tensor([  # 3차원 텐서로 4x1x5 크기를 가진다.\n",
    "    [[1, 2, 3, 4, 5]],\n",
    "    [[1, 2, 3, 4, 5]],\n",
    "    [[1, 2, 3, 4, 5]],\n",
    "    [[1, 2, 3, 4, 5]],\n",
    "])\n",
    "print(a10.shape, a10.ndim)  # 차원은 3이며, 크기는 [4, 1, 5]이다.\n",
    "\n",
    "# 올바르게 크기가 일관된 텐서 생성 예제\n",
    "a11 = torch.tensor([\n",
    "    [[1, 2, 3], [4, 5, 6]],\n",
    "    [[7, 8, 9], [10, 11, 12]],\n",
    "    [[13, 14, 15], [16, 17, 18]],\n",
    "    [[19, 20, 21], [22, 23, 24]],\n",
    "])\n",
    "print(a11.shape, a11.ndim)  # 차원은 4이며, 크기는 [4, 2, 3]이다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2d6bf6-45f6-4bfc-bbae-3ed597acb961",
   "metadata": {},
   "source": [
    "\n",
    "<span style=\"font-size:20px;\"> <span style=\"color:blue;\">b_tensor_initialization_copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38cc0173-09f8-4fe1-bb30-4b12e93817e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3.])\n",
      "tensor([1, 2, 3])\n",
      "tensor([1, 2, 3])\n",
      "####################################################################################################\n",
      "tensor([1., 2., 3.])\n",
      "tensor([1, 2, 3])\n",
      "tensor([100,   2,   3])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 리스트를 사용하여 텐서를 생성\n",
    "l1 = [1, 2, 3]\n",
    "t1 = torch.Tensor(l1)  # torch.Tensor() float32 타입의 텐서를 생성함\n",
    "\n",
    "l2 = [1, 2, 3]\n",
    "t2 = torch.tensor(l2)  # torch.tensor()는 입력값의 dtype을 자동으로 결정 됨\n",
    "\n",
    "l3 = [1, 2, 3]\n",
    "t3 = torch.as_tensor(l3)  # torch.as_tensor()는 입력값을 텐서로 변환하되, 입력 리스트의 변경 사항을 반영함\n",
    "\n",
    "l1[0] = 100  # l1의 첫 번째 요소를 100으로 수정\n",
    "\n",
    "l2[0] = 100  # l2의 첫 번째 요소를 100으로 수정\n",
    "l3[0] = 100  # l3의 첫 번째 요소를 100으로 수정\n",
    "\n",
    "print(t1)  # t1은 l1의 변경 전 상태를 가지고 있음\n",
    "print(t2)  # t2는 l2의 변경 후 상태를 가지고 있음\n",
    "print(t3)  # t3은 l3의 변경 후 상태를 가지고 있음\n",
    "\n",
    "print(\"#\" * 100)\n",
    "\n",
    "# numpy 배열을 사용하여 텐서를 생성함\n",
    "l4 = np.array([1, 2, 3])\n",
    "t4 = torch.Tensor(l4)  # numpy 배열을 torch.Tensor()로 변환하면 float32 텐서가 됨\n",
    "\n",
    "l5 = np.array([1, 2, 3])\n",
    "t5 = torch.tensor(l5)  # numpy 배열을 torch.tensor()로 변환하면 int64 텐서가 됨\n",
    "\n",
    "l6 = np.array([1, 2, 3])\n",
    "t6 = torch.as_tensor(l6)  # numpy 배열을 torch.as_tensor()로 변환하면 원본 배열의 변경 사항을 반영한다\n",
    "\n",
    "l4[0] = 100  # l4의 첫 번째 요소를 100으로 수정\n",
    "\n",
    "l5[0] = 100  # l5의 첫 번째 요소를 100으로 수정\n",
    "l6[0] = 100  # l6의 첫 번째 요소를 100으로 수정\n",
    "\n",
    "print(t4)  # t4는 l4의 변경 전 상태를 가지고 있음 (float32 텐서)\n",
    "print(t5)  # t5는 l5의 변경 후 상태를 가지고 있음 (int64 텐서).\n",
    "print(t6)  # t6은 l6의 변경 후 상태를 가지고 있음 (변경된 값이 반영됨).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba64381c-8b41-4821-a8f1-108d757cd7a5",
   "metadata": {},
   "source": [
    "<span style=\"font-size:20px;\"> <span style=\"color:blue;\">c_tensor_initialization_constant_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "da6a6aed-9709-49fd-a0e2-7a37515c3018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "tensor([0., 0., 0., 0., 0., 0.])\n",
      "tensor([0., 0., 0., 0., 0., 0.])\n",
      "tensor([0., nan, 0., 0.])\n",
      "tensor([1., 2., 3., 0.])\n",
      "tensor([[1., 0., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 0., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# 모든 요소가 1로 초기화된 1차원 텐서를 생성한다\n",
    "\n",
    "t1 = torch.ones(size=(5,))  # 또는 torch.ones(5)와 동일\n",
    "t1_like = torch.ones_like(input=t1)  # t1과 동일한 크기와 데이터 타입을 가진 텐서를 생성, 값은 모두 1\n",
    "print(t1)  \n",
    "print(t1_like) \n",
    "\n",
    "# 모든 요소가 0으로 초기화된 1차원 텐서를 생성한다\n",
    "t2 = torch.zeros(size=(6,))  # 또는 torch.zeros(6)와 동일\n",
    "t2_like = torch.zeros_like(input=t2)  # t2와 동일한 크기와 데이터 타입을 가진 텐서를 생성, 값은 모두 0\n",
    "print(t2)  \n",
    "print(t2_like)  \n",
    "\n",
    "# 비어 있는(초기화되지 않은) 1차원 텐서를 생성한다.\n",
    "t3 = torch.empty(size=(4,))  # 또는 torch.empty(4)와 동일\n",
    "t3_like = torch.empty_like(input=t3)  # t3과 동일한 크기와 데이터 타입을 가진 텐서를 생성, 값은 초기화되지 않음\n",
    "print(t3)  # 값이 초기화되지 않아 임의의 값을 가질 수 있음\n",
    "print(t3_like)  # 값이 초기화되지 않아 임의의 값을 가질 수 있음\n",
    "\n",
    "# 대각선 요소가 1로 초기화된 단위 행렬을 생성한다\n",
    "t4 = torch.eye(n=3)  # 3x3 크기의 단위 행렬을 생성한다\n",
    "print(t4)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6336cadc-7c28-4e6e-a0e8-13e42d5b633d",
   "metadata": {},
   "source": [
    "<span style=\"font-size:20px; color:blue;\"> d_tensor_initialization_random_valuesues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a704bfb-14af-47e3-a38f-9d1da236b9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# low (10) 이상 high (20) 미만의 정수로 구성된 텐서를 생성\n",
    "t1 = torch.randint(low=10, high=20, size=(1, 2))\n",
    "print(t1)  \n",
    "\n",
    "# 0과 1 사이의 균일한 분포에서 랜덤한 값을 가진 텐서를 생성\n",
    "t2 = torch.rand(size=(1, 3))\n",
    "print(t2)  \n",
    "\n",
    "# 정규 분포(평균 0, 표준편차 1)에서 랜덤한 값을 가진 텐서를 생성\n",
    "t3 = torch.randn(size=(1, 3))\n",
    "print(t3)  \n",
    "\n",
    "# 평균이 10, 표준편차가 1인 정규 분포에서 랜덤한 값을 가진 텐서를 생성\n",
    "t4 = torch.normal(mean=10.0, std=1.0, size=(3, 2))\n",
    "print(t4) \n",
    "\n",
    "# start에서 end까지 균일하게 분포된 값을 가지는 텐서를 생성\n",
    "t5 = torch.linspace(start=0.0, end=5.0, steps=3)\n",
    "print(t5)\n",
    "\n",
    "# 0부터 시작하여 지정된 값까지의 정수로 구성된 텐서를 생성\n",
    "t6 = torch.arange(5)\n",
    "print(t6) \n",
    "\n",
    "print(\"#\" * 30)\n",
    "\n",
    "# 랜덤 시드를 설정하여 결과의 재현성을 보장\n",
    "torch.manual_seed(1729)\n",
    "random1 = torch.rand(2, 3)\n",
    "print(random1) \n",
    "\n",
    "random2 = torch.rand(2, 3)\n",
    "print(random2) \n",
    "\n",
    "print()\n",
    "\n",
    "# 동일한 시드를 사용하여 생성된 텐서들이 같은 값을 가진다\n",
    "torch.manual_seed(1729)\n",
    "random3 = torch.rand(2, 3)\n",
    "print(random3)  \n",
    "\n",
    "random4 = torch.rand(2, 3)\n",
    "print(random4) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81eb5b7-f744-4b9f-a290-11a70be1e710",
   "metadata": {},
   "source": [
    "<span style=\"font-size:20px; color:blue;\"> e_tensor_type_conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "18f443c7-ca78-4146-b41c-6fc48ab48f74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "tensor([[1, 1, 1],\n",
      "        [1, 1, 1]], dtype=torch.int16)\n",
      "tensor([[ 4.4800,  0.6582, 15.9762],\n",
      "        [11.1923, 14.8928,  6.7238]], dtype=torch.float64)\n",
      "tensor([[1, 1, 1],\n",
      "        [1, 1, 1]], dtype=torch.int32)\n",
      "torch.float64\n",
      "torch.int16\n",
      "torch.float64\n"
     ]
    }
   ],
   "source": [
    "# 기본적으로 float32 타입으로 2x3 텐서를 생성\n",
    "a = torch.ones((2, 3))\n",
    "print(a.dtype)  # 텐서의 데이터 타입은 float32이다\n",
    "\n",
    "# int16 타입으로 2x3 텐서를 생성\n",
    "b = torch.ones((2, 3), dtype=torch.int16)\n",
    "print(b)  # 이 텐서는 int16 타입으로, 값은 모두 1이다\n",
    "\n",
    "# float64 타입의 랜덤 값을 가지는 2x3 텐서를 생성하고, 모든 값을 20.0으로 스케일링한다\n",
    "c = torch.rand((2, 3), dtype=torch.float64) * 20.\n",
    "print(c)  # c 텐서는 float64 타입이고, 랜덤 값에 20.0을 곱한 결과\n",
    "\n",
    "# int16 타입의 텐서를 int32 타입으로 변환한다\n",
    "d = b.to(torch.int32)\n",
    "print(d)  # d 텐서는 int32 타입으로 변환된 텐서이다.\n",
    "\n",
    "# double (float64) 타입으로 초기화된 10x2 텐서를 생성\n",
    "double_d = torch.ones(10, 2, dtype=torch.double)\n",
    "\n",
    "# short (int16) 타입으로 초기화된 1x2 텐서를 생성\n",
    "short_e = torch.tensor([[1, 2]], dtype=torch.short)\n",
    "\n",
    "# 10x2 텐서를 생성하고 double (float64) 타입으로 변환한다\n",
    "double_d = torch.zeros(10, 2).double()\n",
    "\n",
    "# 10x2 텐서를 생성하고 short (int16) 타입으로 변환한다\n",
    "short_e = torch.ones(10, 2).short()\n",
    "\n",
    "# 10x2 텐서를 생성하고 double (float64) 타입으로 변환한다\n",
    "double_d = torch.zeros(10, 2).to(torch.double)\n",
    "\n",
    "# 10x2 텐서를 생성하고 short (int16) 타입으로 변환한다\n",
    "short_e = torch.ones(10, 2).to(dtype=torch.short)\n",
    "\n",
    "# double (float64) 타입과 short (int16) 타입의 텐서의 데이터 타입을 출력\n",
    "print(double_d.dtype)  # double_d의 데이터 타입은 float64\n",
    "print(short_e.dtype)  # short_e의 데이터 타입은 int16\n",
    "\n",
    "# float64 타입의 랜덤 1차원 텐서를 생성\n",
    "double_f = torch.rand(5, dtype=torch.double)\n",
    "\n",
    "# float64 타입의 텐서를 int16 타입으로 변환한다\n",
    "short_g = double_f.to(torch.short)\n",
    "\n",
    "# double (float64) 타입 텐서와 short타입 텐서의 곱의 데이터 타입을 출력\n",
    "print((double_f * short_g).dtype)  # 곱셈 결과의 데이터 타입은 float32이다\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37a3888-efd9-43e0-bd25-a90e7cc0754f",
   "metadata": {},
   "source": [
    "<span style=\"font-size:20px; color:blue;\"> f_tensor_operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "5e15d8b8-8870-4951-8dee-d3e26850be3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 2., 2.],\n",
      "        [2., 2., 2.]])\n",
      "tensor([[2., 2., 2.],\n",
      "        [2., 2., 2.]])\n",
      "##############################\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "##############################\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "##############################\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# 2x3 크기의 모든 값이 1인 텐서를 생성\n",
    "t1 = torch.ones(size=(2, 3))\n",
    "t2 = torch.ones(size=(2, 3))\n",
    "\n",
    "# 두 텐서를 더합니다. 두 방법 모두 동일한 결과를 생성\n",
    "t3 = torch.add(t1, t2)  # torch.add() 함수 사용\n",
    "t4 = t1 + t2           # + 연산자 사용\n",
    "print(t3)  \n",
    "print(t4)  \n",
    "\n",
    "print(\"#\" * 30)\n",
    "\n",
    "# 두 텐서를 뺀다. 두 방법 모두 동일한 결과를 생성\n",
    "t5 = torch.sub(t1, t2)  # torch.sub() 함수 사용\n",
    "t6 = t1 - t2           # - 연산자 사용\n",
    "print(t5)  \n",
    "print(t6) \n",
    "\n",
    "print(\"#\" * 30)\n",
    "\n",
    "# 두 텐서를 곱한다. 두 방법 모두 동일한 결과를 생성\n",
    "t7 = torch.mul(t1, t2)  # torch.mul() 함수 사용\n",
    "t8 = t1 * t2           # * 연산자 사용\n",
    "print(t7)  \n",
    "print(t8) \n",
    "\n",
    "print(\"#\" * 30)\n",
    "\n",
    "# 두 텐서를 나눈다. 두 방법 모두 동일한 결과를 생성\n",
    "t9 = torch.div(t1, t2)  # torch.div() 함수 사용\n",
    "t10 = t1 / t2          # / 연산자 사용\n",
    "print(t9)  \n",
    "print(t10) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97f6dd0-744c-45f9-bf95-8604e21b8042",
   "metadata": {},
   "source": [
    "<span style=\"font-size:20px; color:blue;\"> g_tensor_operations_mm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e4d1bb-2b26-4a4d-9665-065e5b558ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 두 벡터의 내적을 계산합니다. 두 텐서 모두 1D 텐서\n",
    "t1 = torch.dot(\n",
    "  torch.tensor([2, 3]), torch.tensor([2, 1])\n",
    ")\n",
    "print(t1, t1.size())  # 출력: tensor(7) torch.Size([]), 결과는 스칼라 값\n",
    "\n",
    "# 두 행렬의 곱을 계산\n",
    "t2 = torch.randn(2, 3)  # 2x3 크기의 랜덤 텐서\n",
    "t3 = torch.randn(3, 2)  # 3x2 크기의 랜덤 텐서\n",
    "t4 = torch.mm(t2, t3)  # 행렬 곱셈\n",
    "print(t4, t4.size())  \n",
    "\n",
    "# 배치 행렬 곱셈을 수행\n",
    "t5 = torch.randn(10, 3, 4)  # 배치 차원 10, 3x4 크기의 텐서\n",
    "t6 = torch.randn(10, 4, 5)  # 배치 차원 10, 4x5 크기의 텐서\n",
    "t7 = torch.bmm(t5, t6)  # 배치 행렬 곱셈\n",
    "print(t7.size())  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4882d140-a7f0-41a2-a7c6-01f51672e89b",
   "metadata": {},
   "source": [
    "<span style=\"font-size:20px; color:blue;\"> h_tensor_operations_matmul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a9c6be2c-6707-40bf-9e2a-e18718be43a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([])\n",
      "torch.Size([3])\n",
      "torch.Size([10, 3])\n",
      "torch.Size([10, 3, 5])\n",
      "torch.Size([10, 3, 5])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 벡터와 벡터의 내적 (dot product) 계산\n",
    "t1 = torch.randn(3)  # 크기 3인 랜덤 벡터\n",
    "t2 = torch.randn(3)  # 크기 3인 랜덤 벡터\n",
    "print(torch.matmul(t1, t2).size())  #  스칼라 값)\n",
    "\n",
    "# 행렬과 벡터의 내적 (broadcasted dot) 계산\n",
    "t3 = torch.randn(3, 4)  # 크기 (3, 4)인 랜덤 행렬\n",
    "t4 = torch.randn(4)  # 크기 4의 랜덤 벡터\n",
    "print(torch.matmul(t3, t4).size())  # 각 행에 대해 벡터와의 내적 결과)\n",
    "\n",
    "# 배치된 행렬과 벡터의 내적 (broadcasted dot) 계산\n",
    "t5 = torch.randn(10, 3, 4)  # 크기 (10, 3, 4)인 랜덤 행렬 (배치 크기 10)\n",
    "t6 = torch.randn(4)  # 크기 4의 랜덤 벡터\n",
    "print(torch.matmul(t5, t6).size())  # 배치 크기 10, 각 행에 대해 벡터와의 내적 결과\n",
    "\n",
    "# 배치된 행렬과 배치된 행렬의 행렬 곱 (bmm) 계산\n",
    "t7 = torch.randn(10, 3, 4)  # 크기 (10, 3, 4)인 랜덤 행렬 (배치 크기 10)\n",
    "t8 = torch.randn(10, 4, 5)  # 크기 (10, 4, 5)인 랜덤 행렬 (배치 크기 10)\n",
    "print(torch.matmul(t7, t8).size())  # 배치 크기 10, 각 행렬의 곱 결과\n",
    "\n",
    "# 배치된 행렬과 행렬의 행렬 곱 (bmm) 계산\n",
    "t9 = torch.randn(10, 3, 4)  # 크기 (10, 3, 4)인 랜덤 행렬 (배치 크기 10)\n",
    "t10 = torch.randn(4, 5)  # 크기 (4, 5)인 랜덤 행렬\n",
    "print(torch.matmul(t9, t10).size())  # 배치 크기 10, 각 행렬의 곱 결과\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0776f05a-94a3-4488-abd7-6a39ce5caf2a",
   "metadata": {},
   "source": [
    "<span style=\"font-size:20px; color:blue;\"> i_tensor_broadcasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0fcd0f83-8711-454a-bf7f-e6402ba05901",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 4., 6.])\n",
      "################################################## 1\n",
      "tensor([[-4, -4],\n",
      "        [-2, -1],\n",
      "        [ 6,  5]])\n",
      "################################################## 2\n",
      "tensor([[3., 4.],\n",
      "        [5., 6.]])\n",
      "tensor([[-1.,  0.],\n",
      "        [ 1.,  2.]])\n",
      "tensor([[2., 4.],\n",
      "        [6., 8.]])\n",
      "tensor([[0.5000, 1.0000],\n",
      "        [1.5000, 2.0000]])\n",
      "################################################## 3\n",
      "torch.Size([3, 28, 28])\n",
      "################################################## 4\n",
      "tensor([[4, 3],\n",
      "        [3, 4]])\n",
      "tensor([[6, 7],\n",
      "        [2, 5]])\n",
      "tensor([[8, 6],\n",
      "        [5, 3]])\n",
      "tensor([[ 8,  9],\n",
      "        [ 7, 10]])\n",
      "################################################## 5\n",
      "torch.Size([4, 3, 2])\n",
      "torch.Size([4, 3, 2])\n",
      "torch.Size([4, 3, 2])\n",
      "torch.Size([5, 3, 4, 1])\n",
      "################################################## 6\n",
      "torch.Size([5, 3, 4, 1])\n",
      "torch.Size([3, 1, 7])\n",
      "torch.Size([3, 3, 3])\n",
      "################################################## 7\n",
      "tensor([5., 5., 5., 5.])\n",
      "tensor([25., 25., 25., 25.])\n",
      "tensor([  1.,   4.,  27., 256.])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 1D 텐서와 스칼라 값의 곱셈을 수행\n",
    "t1 = torch.tensor([1.0, 2.0, 3.0])\n",
    "t2 = 2.0\n",
    "print(t1 * t2)  # 각 요소에 스칼라 값을 곱한 결과를 출력한다\n",
    "\n",
    "print(\"#\" * 50, 1)\n",
    "\n",
    "# 2D 텐서와 1D 텐서의 차를 계산한다. Broadcasting이 적용\n",
    "t3 = torch.tensor([[0, 1], [2, 4], [10, 10]])\n",
    "t4 = torch.tensor([4, 5])\n",
    "print(t3 - t4)  # 각 행에서 1D 텐서의 요소를 빼는 결과를 출력한다\n",
    "\n",
    "print(\"#\" * 50, 2)\n",
    "\n",
    "# 2D 텐서에 스칼라 값을 더하거나 빼거나 곱하거나 나눈다. 모든 연산에서 broadcasting이 적용\n",
    "t5 = torch.tensor([[1., 2.], [3., 4.]])\n",
    "print(t5 + 2.0)  # 스칼라 값을 더한 결과\n",
    "print(t5 - 2.0)  # 스칼라 값을 뺀 결과\n",
    "print(t5 * 2.0)  # 스칼라 값으로 곱한 결과\n",
    "print(t5 / 2.0)  # 스칼라 값으로 나눈 결과\n",
    "\n",
    "print(\"#\" * 50, 3)\n",
    "\n",
    "# Normalize 함수는 255로 나누어 값을 정규화한다\n",
    "def normalize(x):\n",
    "  return x / 255\n",
    "\n",
    "# 3D 텐서에 대해 normalize 함수를 적용\n",
    "t6 = torch.randn(3, 28, 28)\n",
    "print(normalize(t6).size())  # 정규화된 텐서의 크기를 출력\n",
    "\n",
    "print(\"#\" * 50, 4)\n",
    "\n",
    "# Broadcasting을 사용하여 텐서의 차원을 맞춰 연산을 수행한다\n",
    "t7 = torch.tensor([[1, 2], [0, 3]])  # torch.Size([2, 2])\n",
    "t8 = torch.tensor([[3, 1]])  # torch.Size([1, 2])\n",
    "t9 = torch.tensor([[5], [2]])  # torch.Size([2, 1])\n",
    "t10 = torch.tensor([7])  # torch.Size([1])\n",
    "print(t7 + t8)   # Broadcasting을 통해 t8을 t7에 더한 결과\n",
    "print(t7 + t9)   # Broadcasting을 통해 t9을 t7에 더한 결과\n",
    "print(t8 + t9)   # Broadcasting을 통해 t9을 t8에 더한 결과\n",
    "print(t7 + t10)  # Broadcasting을 통해 t10을 t7에 더한 결과\n",
    "\n",
    "print(\"#\" * 50, 5)\n",
    "\n",
    "# Broadcasting을 통해 차원이 맞춰진 텐서의 연산을 수행한다\n",
    "t11 = torch.ones(4, 3, 2)\n",
    "t12 = t11 * torch.rand(3, 2)  # 3x2 텐서가 t11의 첫 번째 차원에 broadcast됨\n",
    "print(t12.shape)  # t12의 크기 출력\n",
    "\n",
    "t13 = torch.ones(4, 3, 2)\n",
    "t14 = t13 * torch.rand(3, 1)  # 3x1 텐서가 t13의 두 번째 차원에 broadcast됨\n",
    "print(t14.shape)  # t14의 크기 출력\n",
    "\n",
    "t15 = torch.ones(4, 3, 2)\n",
    "t16 = t15 * torch.rand(1, 2)  # 1x2 텐서가 t15의 첫 번째 차원에 broadcast됨\n",
    "print(t16.shape)  # t16의 크기 출력\n",
    "\n",
    "t17 = torch.ones(5, 3, 4, 1)\n",
    "t18 = torch.rand(3, 1, 1)  # 3x1x1 텐서가 t17의 두 번째 차원에 broadcast됨\n",
    "print((t17 + t18).size())  # 연산 결과 텐서의 크기 출력\n",
    "\n",
    "print(\"#\" * 50, 6)\n",
    "\n",
    "# Broadcasting을 통해 차원이 맞춰진 텐서의 연산을 수행한다\n",
    "t19 = torch.empty(5, 1, 4, 1)\n",
    "t20 = torch.empty(3, 1, 1)\n",
    "print((t19 + t20).size())  \n",
    "\n",
    "t21 = torch.empty(1)\n",
    "t22 = torch.empty(3, 1, 7)\n",
    "print((t21 + t22).size())  \n",
    "\n",
    "t23 = torch.ones(3, 3, 3)\n",
    "t24 = torch.ones(3, 1, 3)\n",
    "print((t23 + t24).size())  \n",
    "\n",
    "# 주석 처리된 코드는 크기 불일치로 인해 실행되지 않음\n",
    "# t25 = torch.empty(5, 2, 4, 1)\n",
    "# t26 = torch.empty(3, 1, 1)\n",
    "# print((t25 + t26).size())\n",
    "\n",
    "\n",
    "print(\"#\" * 50, 7)\n",
    "\n",
    "# 텐서 연산 및 제곱 연산을 수행합니다.\n",
    "t27 = torch.ones(4) * 5\n",
    "print(t27)  # 각 요소가 5인 1D 텐서를 출력합니다.\n",
    "\n",
    "t28 = torch.pow(t27, 2)\n",
    "print(t28)  # 각 요소의 제곱을 출력합니다.\n",
    "\n",
    "exp = torch.arange(1., 5.)  # 1부터 4까지의 연속된 숫자로 텐서를 생성합니다.\n",
    "a = torch.arange(1., 5.)  # 1부터 4까지의 연속된 숫자로 텐서를 생성합니다.\n",
    "t29 = torch.pow(a, exp)  # 각 요소를 exp 텐서의 해당 요소로 제곱한 결과를 출력합니다.\n",
    "print(t29)  # 각 요소가 exp의 해당 요소로 제곱된 결과입니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35d23e1-a2ca-4cf9-aec3-f14a037e69bb",
   "metadata": {},
   "source": [
    "<span style=\"font-size:20px; color:blue;\">j_tensor_indexing_slicing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5b4f5b8b-e9e6-4936-a28f-c93169d4b0d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5, 6, 7, 8, 9])\n",
      "tensor([ 1,  6, 11])\n",
      "tensor(7)\n",
      "tensor([ 4,  9, 14])\n",
      "################################################## 1\n",
      "tensor([[ 5,  6,  7,  8,  9],\n",
      "        [10, 11, 12, 13, 14]])\n",
      "tensor([[ 8,  9],\n",
      "        [13, 14]])\n",
      "################################################## 2\n",
      "tensor([[0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.]])\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]])\n",
      "################################################## 3\n",
      "tensor([[1, 2, 3, 4],\n",
      "        [2, 3, 4, 5]])\n",
      "tensor([[3, 4],\n",
      "        [6, 7]])\n",
      "tensor([[2, 3, 4],\n",
      "        [3, 4, 5],\n",
      "        [6, 7, 8]])\n",
      "tensor([[1, 2, 3, 4],\n",
      "        [2, 0, 0, 5],\n",
      "        [5, 0, 0, 8]])\n"
     ]
    }
   ],
   "source": [
    "# 2D 텐서를 정의\n",
    "x = torch.tensor(\n",
    "  [[0, 1, 2, 3, 4],\n",
    "   [5, 6, 7, 8, 9],\n",
    "   [10, 11, 12, 13, 14]]\n",
    ")\n",
    "\n",
    "# 슬라이싱 및 인덱싱 예시\n",
    "print(x[1])  # 두 번째 행을 선택하여 출력\n",
    "print(x[:, 1])  # 모든 행에서 두 번째 열의 요소를 선택하여 출력\n",
    "print(x[1, 2])  # 두 번째 행과 세 번째 열의 단일 요소를 선택하여 출력\n",
    "print(x[:, -1])  # 모든 행에서 마지막 열의 요소를 선택하여 출력\n",
    "\n",
    "print(\"#\" * 50, 1)\n",
    "\n",
    "print(x[1:])  # 두 번째 행부터 끝까지 선택하여 출력\n",
    "print(x[1:, 3:])  # 두 번째 행부터 끝까지, 네 번째 열부터 끝까지 선택하여 출력\n",
    "\n",
    "print(\"#\" * 50, 2)\n",
    "\n",
    "# 6x6 크기의 제로 텐서를 정의\n",
    "y = torch.zeros((6, 6))\n",
    "y[1:4, 2] = 1  # 두 번째부터 네 번째 행까지의 세 번째 열을 1로 설정\n",
    "print(y)  # 수정된 텐서를 출력\n",
    "\n",
    "print(y[1:4, 1:4])  # 두 번째부터 네 번째 행까지, 두 번째부터 네 번째 열까지의 부분 텐서를 출력\n",
    "\n",
    "print(\"#\" * 50, 3)\n",
    "\n",
    "# 2D 텐서를 정의\n",
    "z = torch.tensor(\n",
    "  [[1, 2, 3, 4],\n",
    "   [2, 3, 4, 5],\n",
    "   [5, 6, 7, 8]]\n",
    ")\n",
    "print(z[:2])  # 첫 번째와 두 번째 행을 선택하여 출력\n",
    "print(z[1:, 1:3])  # 두 번째 행부터 끝까지, 두 번째부터 세 번째 열까지 선택하여 출력\n",
    "print(z[:, 1:])  # 모든 행에서 두 번째 열부터 끝까지 선택하여 출력\n",
    "\n",
    "# 특정 부분을 수정\n",
    "z[1:, 1:3] = 0  # 두 번째 행부터 끝까지, 두 번째부터 세 번째 열까지를 0으로 설정\n",
    "print(z)  # 수정된 텐서를 출력\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa80e18-d66a-4efc-85bf-847626c1e167",
   "metadata": {},
   "source": [
    "<span style=\"font-size:20px; color:blue;\"> k_tensor_reshaping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "05397e93-9d48-447b-99e4-712558c43653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2],\n",
      "        [3, 4],\n",
      "        [5, 6]])\n",
      "tensor([[1, 2, 3, 4, 5, 6]])\n",
      "tensor([[0, 1, 2, 3],\n",
      "        [4, 5, 6, 7]])\n",
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5]])\n",
      "################################################## 1\n",
      "tensor([1, 2, 3])\n",
      "tensor([[1],\n",
      "        [2],\n",
      "        [3]])\n",
      "################################################## 2\n",
      "tensor([[1],\n",
      "        [2],\n",
      "        [3]])\n",
      "tensor([[[1, 2, 3]],\n",
      "\n",
      "        [[4, 5, 6]]]) torch.Size([2, 1, 3])\n",
      "################################################## 3\n",
      "tensor([1, 2, 3, 4, 5, 6])\n",
      "tensor([1, 2, 3, 4, 5, 6, 7, 8])\n",
      "tensor([[1, 2, 3, 4],\n",
      "        [5, 6, 7, 8]])\n",
      "################################################## 4\n",
      "torch.Size([2, 3, 5])\n",
      "torch.Size([5, 2, 3])\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "tensor([[1, 4],\n",
      "        [2, 5],\n",
      "        [3, 6]])\n",
      "tensor([[1, 4],\n",
      "        [2, 5],\n",
      "        [3, 6]])\n",
      "tensor([[1, 4],\n",
      "        [2, 5],\n",
      "        [3, 6]])\n"
     ]
    }
   ],
   "source": [
    "# 텐서의 크기를 변경하는 예시\n",
    "\n",
    "t1 = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "t2 = t1.view(3, 2)  # 2x3 크기의 텐서를 3x2 크기로 변경\n",
    "t3 = t1.reshape(1, 6)  # 2x3 크기의 텐서를 1x6 크기로 변경\n",
    "print(t2)\n",
    "print(t3)\n",
    "\n",
    "# 텐서를 생성하고 view를 사용하여 모양을 변경\n",
    "t4 = torch.arange(8).view(2, 4)  # 8개의 값을 2x4 크기로 보기\n",
    "t5 = torch.arange(6).view(2, 3)  # 6개의 값을 2x3 크기로 보기\n",
    "print(t4)\n",
    "print(t5)\n",
    "\n",
    "print(\"#\" * 50, 1)\n",
    "\n",
    "# squeeze()와 unsqueeze() 사용 예시\n",
    "# squeeze(): 크기가 1인 차원을 제거\n",
    "t6 = torch.tensor([[[1], [2], [3]]])  # (1, 3, 1) 크기의 텐서 생성\n",
    "t7 = t6.squeeze()  # 크기가 1인 차원을 모두 제거, 결과는 (3,)\n",
    "t8 = t6.squeeze(0)  # 첫 번째 차원만 제거, 결과 : (3, 1)\n",
    "print(t7)\n",
    "print(t8)\n",
    "\n",
    "print(\"#\" * 50, 2)\n",
    "\n",
    "# unsqueeze(): 새로운 차원을 추가\n",
    "t9 = torch.tensor([1, 2, 3])  # (3,) 크기의 텐서\n",
    "t10 = t9.unsqueeze(1)  # 두 번째 차원에 새 축 추가, 결과 : (3, 1)\n",
    "print(t10)\n",
    "\n",
    "t11 = torch.tensor(\n",
    "  [[1, 2, 3],\n",
    "   [4, 5, 6]]\n",
    ")\n",
    "t12 = t11.unsqueeze(1)  # 두 번째 차원에 새 축 추가, 결과 : (2, 1, 3)\n",
    "print(t12, t12.shape)\n",
    "\n",
    "print(\"#\" * 50, 3)\n",
    "\n",
    "# flatten()\n",
    "t13 = torch.tensor([[1, 2, 3], [4, 5, 6]])  # (2, 3) 크기의 텐서\n",
    "t14 = t13.flatten()  # 텐서를 1차원으로 평탄화, 결과 : (6,)\n",
    "print(t14)\n",
    "\n",
    "# 복잡한 텐서를 평탄화\n",
    "t15 = torch.tensor([[[1, 2],\n",
    "                     [3, 4]],\n",
    "                    [[5, 6],\n",
    "                     [7, 8]]])  # (2, 2, 2) 크기의 텐서\n",
    "t16 = torch.flatten(t15)  # 전체를 평탄화하여 (8,)\n",
    "t17 = torch.flatten(t15, start_dim=1)  # 첫 번째 차원 제외하고 평탄화, 결과 : (2, 4)\n",
    "print(t16)\n",
    "print(t17)\n",
    "\n",
    "print(\"#\" * 50, 4)\n",
    "\n",
    "# permute()  텐서의 차원을 재배치\n",
    "t18 = torch.randn(2, 3, 5)  # (2, 3, 5) 크기의 랜덤 텐서\n",
    "print(t18.shape)\n",
    "print(torch.permute(t18, (2, 0, 1)).size())  # (5, 2, 3)으로 차원 재배치\n",
    "\n",
    "# permute와 transpose 차이\n",
    "t19 = torch.tensor([[1, 2, 3], [4, 5, 6]])  # (2, 3) 크기의 텐서\n",
    "t20 = torch.permute(t19, dims=(0, 1))  # 순서 그대로, 크기 변화 없음\n",
    "t21 = torch.permute(t19, dims=(1, 0))  # 차원 재배치하여 (3, 2)\n",
    "print(t20)\n",
    "print(t21)\n",
    "\n",
    "# transpose  두 차원의 위치를 변경\n",
    "t22 = torch.transpose(t19, 0, 1)  # (3, 2)로 차원 교환\n",
    "print(t22)\n",
    "\n",
    "# t()를 사용하여 2D 텐서를 전치\n",
    "t23 = torch.t(t19)  # 2D 텐서의 전치, 결과 : (3, 2)\n",
    "print(t23)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02003b8-af44-4fbf-bf65-8231ad6475af",
   "metadata": {},
   "source": [
    "<span style=\"font-size:20px; color:blue;\">l_tensor_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "67cf67d6-0a4e-41e3-9281-067a1a898ccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 3])\n",
      "################################################## 1\n",
      "torch.Size([8])\n",
      "tensor([0, 1, 2, 3, 4, 5, 6, 7])\n",
      "################################################## 2\n",
      "torch.Size([4, 3])\n",
      "tensor([[ 0,  1,  2],\n",
      "        [ 3,  4,  5],\n",
      "        [ 6,  7,  8],\n",
      "        [ 9, 10, 11]])\n",
      "torch.Size([2, 6])\n",
      "tensor([[ 0,  1,  2,  6,  7,  8],\n",
      "        [ 3,  4,  5,  9, 10, 11]])\n",
      "################################################## 3\n",
      "torch.Size([6, 3])\n",
      "tensor([[ 0,  1,  2],\n",
      "        [ 3,  4,  5],\n",
      "        [ 6,  7,  8],\n",
      "        [ 9, 10, 11],\n",
      "        [12, 13, 14],\n",
      "        [15, 16, 17]])\n",
      "torch.Size([2, 9])\n",
      "tensor([[ 0,  1,  2,  6,  7,  8, 12, 13, 14],\n",
      "        [ 3,  4,  5,  9, 10, 11, 15, 16, 17]])\n",
      "################################################## 4\n",
      "torch.Size([2, 2, 3])\n",
      "tensor([[[ 0,  1,  2],\n",
      "         [ 3,  4,  5]],\n",
      "\n",
      "        [[ 6,  7,  8],\n",
      "         [ 9, 10, 11]]])\n",
      "torch.Size([1, 4, 3])\n",
      "tensor([[[ 0,  1,  2],\n",
      "         [ 3,  4,  5],\n",
      "         [ 6,  7,  8],\n",
      "         [ 9, 10, 11]]])\n",
      "torch.Size([1, 2, 6])\n",
      "tensor([[[ 0,  1,  2,  6,  7,  8],\n",
      "         [ 3,  4,  5,  9, 10, 11]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 다양한 차원에서 텐서를 이어붙이는 예시\n",
    "# 원본 텐서들\n",
    "t1 = torch.zeros([2, 1, 3])  # (2, 1, 3) 크기의 제로 텐서\n",
    "t2 = torch.zeros([2, 3, 3])  # (2, 3, 3) 크기의 제로 텐서\n",
    "t3 = torch.zeros([2, 2, 3])  # (2, 2, 3) 크기의 제로 텐서\n",
    "\n",
    "# dim=1을 기준으로 텐서 이어붙이기\n",
    "t4 = torch.cat([t1, t2, t3], dim=1)  # (2, 6, 3) 크기로 이어붙임\n",
    "print(t4.shape)\n",
    "\n",
    "print(\"#\" * 50, 1)\n",
    "\n",
    "# 1차원 텐서의 이어붙이기\n",
    "t5 = torch.arange(0, 3)  # tensor([0, 1, 2])\n",
    "t6 = torch.arange(3, 8)  # tensor([3, 4, 5, 6, 7])\n",
    "\n",
    "t7 = torch.cat((t5, t6), dim=0)  # (8,) 크기로 이어붙임\n",
    "print(t7.shape)  # >>> torch.Size([8])\n",
    "print(t7)  \n",
    "\n",
    "print(\"#\" * 50, 2)\n",
    "\n",
    "# 2차원 텐서의 이어붙이기\n",
    "# (2, 3) 크기의 텐서\n",
    "t8 = torch.arange(0, 6).reshape(2, 3)  \n",
    "t9 = torch.arange(6, 12).reshape(2, 3) \n",
    "\n",
    "# 차원 0을 기준으로 이어붙이기\n",
    "t10 = torch.cat((t8, t9), dim=0)  # (4, 3) 크기로 이어붙임\n",
    "print(t10.size())  # >>> torch.Size([4, 3])\n",
    "print(t10)\n",
    "\n",
    "\n",
    "# 차원 1을 기준으로 이어붙이기\n",
    "t11 = torch.cat((t8, t9), dim=1)  # (2, 6) 크기로 이어붙임\n",
    "print(t11.size())  # >>> torch.Size([2, 6])\n",
    "print(t11)\n",
    "\n",
    "print(\"#\" * 50, 3)\n",
    "\n",
    "# 2차원 텐서 3개 이어붙이기\n",
    "# (2, 3) 크기의 텐서\n",
    "t12 = torch.arange(0, 6).reshape(2, 3)  \n",
    "t13 = torch.arange(6, 12).reshape(2, 3) \n",
    "t14 = torch.arange(12, 18).reshape(2, 3)  \n",
    "\n",
    "# 차원 0을 기준으로 이어붙이기\n",
    "t15 = torch.cat((t12, t13, t14), dim=0)  # (6, 3) 크기로 이어붙임\n",
    "print(t15.size())  # >>> torch.Size([6, 3])\n",
    "print(t15)\n",
    "\n",
    "\n",
    "# 차원 1을 기준으로 이어붙이기\n",
    "t16 = torch.cat((t12, t13, t14), dim=1)  # (2, 9) 크기로 이어붙임\n",
    "print(t16.size())  # >>> torch.Size([2, 9])\n",
    "print(t16)\n",
    "\n",
    "\n",
    "print(\"#\" * 50, 4)\n",
    "\n",
    "# 3차원 텐서의 이어붙이기\n",
    "t17 = torch.arange(0, 6).reshape(1, 2, 3)  # (1, 2, 3) 크기의 텐서\n",
    "t18 = torch.arange(6, 12).reshape(1, 2, 3)  # (1, 2, 3) 크기의 텐서\n",
    "\n",
    "# 차원 0을 기준으로 이어붙이기\n",
    "t19 = torch.cat((t17, t18), dim=0)  # (2, 2, 3) 크기로 이어붙임\n",
    "print(t19.size())  # >>> torch.Size([2, 2, 3])\n",
    "print(t19)\n",
    "\n",
    "\n",
    "\n",
    "# 차원 1을 기준으로 이어붙이기\n",
    "t20 = torch.cat((t17, t18), dim=1)  # (1, 4, 3) 크기로 이어붙임\n",
    "print(t20.size())  # >>> torch.Size([1, 4, 3])\n",
    "print(t20)\n",
    "\n",
    "# 차원 2을 기준으로 이어붙이기\n",
    "t21 = torch.cat((t17, t18), dim=2)  # (1, 2, 6) 크기로 이어붙임\n",
    "print(t21.size())  # >>> torch.Size([1, 2, 6])\n",
    "print(t21)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae6cfe8-457e-4941-b3c2-039ff2c9ccf3",
   "metadata": {},
   "source": [
    "<span style=\"font-size:20px; color:blue;\"> m_tensor_stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "91152e46-40ff-4fbf-997a-98f628d58408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2, 3]) True\n",
      "torch.Size([2, 2, 3]) True\n",
      "torch.Size([2, 3, 2]) True\n",
      "################################################## 1\n",
      "torch.Size([3]) torch.Size([3])\n",
      "torch.Size([2, 3])\n",
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5]])\n",
      "True\n",
      "torch.Size([3, 2])\n",
      "tensor([[0, 3],\n",
      "        [1, 4],\n",
      "        [2, 5]])\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 두 개의 2차원 텐서를 정의한다\n",
    "t1 = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "t2 = torch.tensor([[7, 8, 9], [10, 11, 12]])\n",
    "\n",
    "# 텐서 t1과 t2를 새로운 차원(dim=0)으로 스택한다\n",
    "t3 = torch.stack([t1, t2], dim=0)\n",
    "# t1과 t2에 새로운 차원을 추가한 후, 이 차원을 따라 연결한다\n",
    "\n",
    "t4 = torch.cat([t1.unsqueeze(dim=0), t2.unsqueeze(dim=0)], dim=0)\n",
    "# 두 방법 모두 같은 결과를 생성하므로, 둘의 모양과 내용을 비교한다\n",
    "print(t3.shape, t3.equal(t4))  \n",
    "\n",
    "# t1과 t2를 차원 1로 스택한다\n",
    "t5 = torch.stack([t1, t2], dim=1)\n",
    "# t1과 t2에 차원 1을 추가한 후, 이 차원을 따라 연결한다\n",
    "t6 = torch.cat([t1.unsqueeze(dim=1), t2.unsqueeze(dim=1)], dim=1)\n",
    "# 두 방법 모두 같은 결과를 생성하므로, 둘의 모양과 내용을 비교한다\n",
    "print(t5.shape, t5.equal(t6)) #torch.Size([2, 2, 3]) True\n",
    "\n",
    "# t1과 t2를 차원 2로 스택합니다.\n",
    "t7 = torch.stack([t1, t2], dim=2)\n",
    "# t1과 t2에 차원 2를 추가한 후, 이 차원을 따라 연결한다\n",
    "t8 = torch.cat([t1.unsqueeze(dim=2), t2.unsqueeze(dim=2)], dim=2)\n",
    "# 두 방법 모두 같은 결과를 생성하므로, 둘의 모양과 내용을 비교한다\n",
    "print(t7.shape, t7.equal(t8))  \n",
    "print(\"#\" * 50, 1)\n",
    "\n",
    "# 1차원 텐서를 정의합니다.\n",
    "t9 = torch.arange(0, 3)  # tensor([0, 1, 2])\n",
    "t10 = torch.arange(3, 6)  # tensor([3, 4, 5])\n",
    "\n",
    "print(t9.size(), t10.size())\n",
    "# 두 텐서 모두 크기가 [3]\n",
    "\n",
    "# t9와 t10을 새로운 차원(dim=0)으로 스택한다\n",
    "t11 = torch.stack((t9, t10), dim=0)\n",
    "print(t11.size())  \n",
    "print(t11)  \n",
    "\n",
    "# t9와 t10에 차원 0을 추가한 후, 이 차원을 따라 연결한다\n",
    "t12 = torch.cat((t9.unsqueeze(0), t10.unsqueeze(0)), dim=0)\n",
    "print(t11.equal(t12))  \n",
    "\n",
    "# t9와 t10을 차원 1로 스택한다.\n",
    "t13 = torch.stack((t9, t10), dim=1)\n",
    "print(t13.size())  \n",
    "print(t13)\n",
    "\n",
    "# t9와 t10에 차원 1을 추가한 후, 이 차원을 따라 연결한다\n",
    "t14 = torch.cat((t9.unsqueeze(1), t10.unsqueeze(1)), dim=1)\n",
    "print(t13.equal(t14))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2127396-3cbd-4df0-9a74-159967207982",
   "metadata": {},
   "source": [
    "<span style=\"font-size:20px; color:blue;\"> n_tensor_vstack_hstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7751b15b-779a-41a1-9ead-28ba7fca5aea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "tensor([[1],\n",
      "        [2],\n",
      "        [3],\n",
      "        [4],\n",
      "        [5],\n",
      "        [6]])\n",
      "torch.Size([2, 2, 3])\n",
      "torch.Size([2, 2, 3])\n",
      "torch.Size([4, 2, 3])\n",
      "################################################## 1\n",
      "tensor([1, 2, 3, 4, 5, 6])\n",
      "tensor([[1, 4],\n",
      "        [2, 5],\n",
      "        [3, 6]])\n",
      "torch.Size([2, 2, 3])\n",
      "torch.Size([2, 2, 3])\n",
      "torch.Size([2, 4, 3])\n",
      "tensor([[[ 1,  2,  3],\n",
      "         [ 4,  5,  6],\n",
      "         [13, 14, 15],\n",
      "         [16, 17, 18]],\n",
      "\n",
      "        [[ 7,  8,  9],\n",
      "         [10, 11, 12],\n",
      "         [19, 20, 21],\n",
      "         [22, 23, 24]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 1차원 텐서 t1과 t2를 수직으로 스택한다\n",
    "t1 = torch.tensor([1, 2, 3])\n",
    "t2 = torch.tensor([4, 5, 6])\n",
    "t3 = torch.vstack((t1, t2))\n",
    "print(t3)\n",
    "# 출력: tensor([[1, 2, 3],\n",
    "#               [4, 5, 6]])\n",
    "# vstack은 주어진 텐서들을 새로운 차원으로 연결한다.\n",
    "\n",
    "# 2차원 텐서 t4와 t5를 수직으로 스택한다\n",
    "t4 = torch.tensor([[1], [2], [3]])\n",
    "t5 = torch.tensor([[4], [5], [6]])\n",
    "t6 = torch.vstack((t4, t5))\n",
    "print(t6)\n",
    "# 출력: tensor([[1],\n",
    "#               [2],\n",
    "#               [3],\n",
    "#               다\n",
    "\n",
    "# 3차원 텐서 t7과 t8을 수직으로 스택합니다.\n",
    "t7 = torch.tensor([\n",
    "  [[1, 2, 3], [4, 5, 6]],\n",
    "  [[7, 8, 9], [10, 11, 12]]\n",
    "])\n",
    "print(t7.shape)\n",
    "# 출력: (2, 2, 3)\n",
    "\n",
    "t8 = torch.tensor([\n",
    "  [[13, 14, 15], [16, 17, 18]],\n",
    "  [[19, 20, 21], [22, 23, 24]]\n",
    "])\n",
    "print(t8.shape)\n",
    "# 출력: (2, 2, 3)\n",
    "\n",
    "t9 = torch.vstack([t7, t8])\n",
    "print(t9.shape)\n",
    "\n",
    "print(\"#\" * 50, 1)\n",
    "\n",
    "# 1차원 텐서 t10과 t11을 수평으로 스택한다\n",
    "t10 = torch.tensor([1, 2, 3])\n",
    "t11 = torch.tensor([4, 5, 6])\n",
    "t12 = torch.hstack((t10, t11))\n",
    "print(t12)\n",
    "\n",
    "# hstack은 주어진 텐서들을 수평으로 연결한다\n",
    "\n",
    "# 2차원 텐서 t13과 t14를 수평으로 스택한다\n",
    "\n",
    "t13 = torch.tensor([[1], [2], [3]])\n",
    "t14 = torch.tensor([[4], [5], [6]])\n",
    "t15 = torch.hstack((t13, t14))\n",
    "print(t15)\n",
    "\n",
    "# 각 텐서의 두 번째 차원(열)으로 연결된다\n",
    "\n",
    "# 3차원 텐서 t16과 t17을 수평으로 스택한다\n",
    "t16 = torch.tensor([\n",
    "  [[1, 2, 3], [4, 5, 6]],\n",
    "  [[7, 8, 9], [10, 11, 12]]\n",
    "])\n",
    "print(t16.shape)\n",
    "\n",
    "\n",
    "t17 = torch.tensor([\n",
    "  [[13, 14, 15], [16, 17, 18]],\n",
    "  [[19, 20, 21], [22, 23, 24]]\n",
    "])\n",
    "print(t17.shape)\n",
    "\n",
    "\n",
    "t18 = torch.hstack([t16, t17])\n",
    "print(t18.shape)\n",
    "\n",
    "print(t18)\n",
    "\n",
    "# hstack은 텐서들을 수평으로 연결합니다. 여기서는 두 번째 차원으로 연결된다\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
